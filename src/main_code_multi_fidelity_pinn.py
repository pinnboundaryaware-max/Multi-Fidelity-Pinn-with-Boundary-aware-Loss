# -*- coding: utf-8 -*-
"""Main_code_multi_fidelity_PINN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pQGYiZO4gc7DqFBgT6e9O5elpsZkNqDj
"""

# ============================================================
# Cell 1: Imports and Hyperparameters
# ============================================================
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# ============================================================
# Toggles and loss weighting strategy (uncertainty version)
# ------------------------------------------------------------
# NORMALIZE_Y:     If True, target (bed) values are standardized before training.
#
# USE_UNCERTAINTY: If True → ignore fixed weights and let the model learn
#                  adaptive weights via Kendall's uncertainty weighting
#                  (log-variances clamped by LOG_CLAMP).
#
# W_SIA, W_STK:    Ignored when USE_UNCERTAINTY=True.
#
# LAMBDA_NEU, LAMBDA_DIR: Relative weights for Neumann (flux/traction)
#                         and Dirichlet boundary losses.
# ============================================================

NORMALIZE_Y     = True            # normalize target for training (recommended)
USE_UNCERTAINTY = True            # <-- switch on learned Kendall weighting
LOG_CLAMP       = (-3.0, 3.0)     # tighter clamp for stability (optional)

# Physics weights (ignored when USE_UNCERTAINTY=True)
W_SIA, W_STK    = 0.25, 0.75

# Boundary weights
LAMBDA_NEU, LAMBDA_DIR = 0.1, 0.0

# Training schedule
EPOCHS          = 25000
HIDDEN          = 128
LR_INIT, LR_MIN = 3e-3, 3e-4
N_COL_START     = 512
N_COL_END       = 4096
N_BPER          = 96

SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

# ============================================================
# Cell 2: Load Data
# ============================================================
df = pd.read_csv("data_full.csv")

feat_cols = ['surf_x','surf_y','surf_vx','surf_vy','surf_elv','surf_dhdt','surf_SMB']
target_col = 'track_bed_target'

for c in feat_cols + [target_col]:
    if c not in df.columns:
        raise ValueError(f"Missing required column: {c}")

X_np = df[feat_cols].to_numpy(dtype=np.float32)
y_np = df[target_col].to_numpy(dtype=np.float32).reshape(-1,1)

# Split 80/20
n = len(X_np)
idx = np.random.permutation(n)
split = int(0.8*n)
train_idx, test_idx = idx[:split], idx[split:]

x_train_np, x_test_np = X_np[train_idx], X_np[test_idx]
y_train_np, y_test_np = y_np[train_idx], y_np[test_idx]

# Scale X and optionally y
x_scaler = StandardScaler()
X_train = x_scaler.fit_transform(x_train_np)
X_test  = x_scaler.transform(x_test_np)

if NORMALIZE_Y:
    y_scaler = StandardScaler()
    y_train = y_scaler.fit_transform(y_train_np)
    y_test  = y_scaler.transform(y_test_np)
else:
    y_scaler = None
    y_train, y_test = y_train_np, y_test_np

# Tensors
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device)
X_test_tensor  = torch.tensor(X_test,  dtype=torch.float32, device=device)
y_test_tensor  = torch.tensor(y_test,  dtype=torch.float32, device=device)

# Domain bbox (unscaled surf_x, surf_y)
xy_np = df[['surf_x','surf_y']].to_numpy(dtype=np.float32)
xmin, xmax = float(xy_np[:,0].min()), float(xy_np[:,0].max())
ymin, ymax = float(xy_np[:,1].min()), float(xy_np[:,1].max())
bbox = (xmin, xmax, ymin, ymax)

# ============================================================
# Cell 3: Helpers
# ============================================================
def grad(outputs, inputs):
    return torch.autograd.grad(outputs, inputs,
        grad_outputs=torch.ones_like(outputs),
        create_graph=True, retain_graph=True, only_inputs=True)[0]

def laplacian(u, X):
    g = grad(u, X)
    du_dx, du_dy = g[:,0:1], g[:,1:2]
    d2u_dx2 = grad(du_dx, X)[:,0:1]
    d2u_dy2 = grad(du_dy, X)[:,1:2]
    return d2u_dx2 + d2u_dy2

def infer_normals(Xb_xy, bbox):
    xmin, xmax, ymin, ymax = bbox
    n = torch.zeros_like(Xb_xy)
    tol = 1e-6
    left  = torch.isclose(Xb_xy[:,0], torch.tensor(xmin, device=Xb_xy.device), atol=tol)
    right = torch.isclose(Xb_xy[:,0], torch.tensor(xmax, device=Xb_xy.device), atol=tol)
    bot   = torch.isclose(Xb_xy[:,1], torch.tensor(ymin, device=Xb_xy.device), atol=tol)
    top   = torch.isclose(Xb_xy[:,1], torch.tensor(ymax, device=Xb_xy.device), atol=tol)
    n[left,0]  = -1.0; n[right,0] = 1.0
    n[bot,1]   = -1.0; n[top,1]   = 1.0
    return n

# Embed physical (x,y) to scaled feature vector
mu_xy  = torch.tensor(x_scaler.mean_[:2],  dtype=torch.float32, device=device)
sig_xy = torch.tensor(x_scaler.scale_[:2], dtype=torch.float32, device=device)

def embed_xy_scaled(X_xy):
    npts = X_xy.shape[0]
    d = X_train_tensor.shape[1]
    X_full = torch.zeros(npts, d, device=device)
    X_full[:, :2] = (X_xy - mu_xy) / sig_xy
    return X_full

def sample_interior(nc):
    rx = torch.rand(nc, 1, device=device)
    ry = torch.rand(nc, 1, device=device)
    X_xy = torch.cat([xmin + (xmax - xmin)*rx, ymin + (ymax - ymin)*ry], dim=1)
    return embed_xy_scaled(X_xy).requires_grad_(True)

def boundary_collocation(bbox, n_per_side=128):
    xmin, xmax, ymin, ymax = bbox
    xs = torch.linspace(xmin, xmax, n_per_side, device=device)
    ys = torch.linspace(ymin, ymax, n_per_side, device=device)
    top = torch.stack([xs, torch.full_like(xs, ymax)], dim=1)
    bot = torch.stack([xs, torch.full_like(xs, ymin)], dim=1)
    left  = torch.stack([torch.full_like(ys, xmin), ys], dim=1)
    right = torch.stack([torch.full_like(ys, xmax), ys], dim=1)
    Xb_xy = torch.cat([top, bot, left, right], dim=0)
    Nb = infer_normals(Xb_xy, bbox)
    return Xb_xy, Nb

def sample_boundary(n_per_side=N_BPER):
    Xb_xy, Nb = boundary_collocation(bbox, n_per_side=n_per_side)
    return embed_xy_scaled(Xb_xy).requires_grad_(True), Nb

# ============================================================
# Cell 4: Multi-fidelity Residuals and Boundary Loss
# ============================================================
class MultiFidelityResiduals(nn.Module):
    def __init__(self, use_uncertainty_weighting, clamp=LOG_CLAMP, w_sia=1.0, w_stk=1.0):
        super().__init__()
        self.use_unc = use_uncertainty_weighting
        self.log_sigma_sia = nn.Parameter(torch.tensor(0.0))
        self.log_sigma_stk = nn.Parameter(torch.tensor(0.0))
        self.clamp = clamp
        self.w_sia = w_sia
        self.w_stk = w_stk

    def sia_residual(self, model, X, extra=None):
        u = model(X)
        M = extra.get("M") if (extra is not None and "M" in extra) else torch.ones_like(u)
        g = grad(u, X)
        qx, qy = M*g[:,0:1], M*g[:,1:2]
        dqxdx = grad(qx, X)[:,0:1]
        dqydy = grad(qy, X)[:,1:2]
        return dqxdx + dqydy

    def stokes_reduced_residual(self, model, X, body_force=None, nu=1.0):
        u = model(X)
        Lu = -nu * laplacian(u, X)
        f = torch.zeros_like(u) if body_force is None else body_force
        return Lu - f

    def forward(self, model, Xc, extra=None):
        r_sia = self.sia_residual(model, Xc, extra=extra)
        r_stk = self.stokes_reduced_residual(model, Xc, body_force=None, nu=1.0)
        if self.use_unc:
            l1 = torch.clamp(self.log_sigma_sia, *self.clamp)
            l2 = torch.clamp(self.log_sigma_stk, *self.clamp)
            w1, w2 = torch.exp(-l1), torch.exp(-l2)
            loss = 0.5*(w1*(r_sia**2).mean() + l1) + 0.5*(w2*(r_stk**2).mean() + l2)
        else:
            loss = self.w_sia*(r_sia**2).mean() + self.w_stk*(r_stk**2).mean()
        return loss, {"r_sia": r_sia.detach(), "r_stk": r_stk.detach()}

class BoundaryWeakForm(nn.Module):
    def __init__(self, dirichlet_fraction=0.0, hard_dirichlet=False, lambda_dir=LAMBDA_DIR, lambda_neu=LAMBDA_NEU):
        super().__init__()
        self.dir_frac = dirichlet_fraction
        self.hard_dir = hard_dirichlet
        self.lambda_dir = lambda_dir
        self.lambda_neu = lambda_neu

    def neumann_loss(self, model, Xb_full, Nb, gN=None):
        u = model(Xb_full)
        gu = torch.autograd.grad(u, Xb_full, grad_outputs=torch.ones_like(u),
                                 create_graph=True, retain_graph=True)[0]
        gu_xy = gu[:, :2]
        flux = (gu_xy * Nb).sum(dim=1, keepdim=True)
        gN = torch.zeros_like(flux) if gN is None else gN
        return (flux - gN).pow(2).mean()

    def dirichlet_loss(self, model, Xb_full, uD):
        u = model(Xb_full)
        return (u - uD).pow(2).mean()

    def forward(self, model, Xb_full, Nb, uD=None, gN=None):
        n = Xb_full.shape[0]
        n_dir = int(self.dir_frac * n)
        perm = torch.randperm(n, device=Xb_full.device)
        Xb_dir = Xb_full[perm[:n_dir]]
        Xb_neu = Xb_full[perm[n_dir:]]
        Nb_neu = Nb[perm[n_dir:]]
        loss_neu = self.neumann_loss(model, Xb_neu, Nb_neu, gN=gN)
        loss_dir = torch.tensor(0.0, device=Xb_full.device)
        if uD is not None and n_dir > 0:
            uD_sub = uD[perm[:n_dir]]
            if self.hard_dir:
                loss_dir = 100.0 * self.dirichlet_loss(model, Xb_dir, uD_sub)
            else:
                loss_dir = self.dirichlet_loss(model, Xb_dir, uD_sub)
        return self.lambda_neu*loss_neu + self.lambda_dir*loss_dir, {
            "loss_neu": loss_neu.detach(),
            "loss_dir": loss_dir.detach()
        }

# ============================================================
# Cell 5: Model, Optimizer, and Training Loop
# ============================================================
class PINN_Model(nn.Module):
    def __init__(self, input_dim, hidden, output_dim=1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, hidden), nn.Tanh(),
            nn.Linear(hidden, output_dim)
        )
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)
    def forward(self, x): return self.net(x)

model = PINN_Model(input_dim=X_train_tensor.shape[1], hidden=HIDDEN).to(device)
mf = MultiFidelityResiduals(USE_UNCERTAINTY, w_sia=W_SIA, w_stk=W_STK).to(device)
bw = BoundaryWeakForm().to(device)

optimizer = optim.Adam(model.parameters(), lr=LR_INIT)
mse = nn.MSELoss()

uD_full, gN_full = None, None

# Cosine LR
import math
def cosine_lr(it, total, lr_min=LR_MIN, lr_max=LR_INIT):
    return lr_min + 0.5*(lr_max - lr_min)*(1 + math.cos(math.pi*min(it,total)/total))

# Training loop
for epoch in range(1, EPOCHS+1):
    t = epoch / EPOCHS
    N_COLLOCATION = int(N_COL_START + t*(N_COL_END - N_COL_START))
    lr = cosine_lr(epoch, EPOCHS)
    for pg in optimizer.param_groups: pg['lr'] = lr

    model.train(); optimizer.zero_grad()
    y_pred = model(X_train_tensor)
    loss_data = mse(y_pred, y_train_tensor)

    Xc = sample_interior(N_COLLOCATION)
    loss_phys, phys_stats = mf(model, Xc, extra=None)

    Xb_full, Nb = sample_boundary(n_per_side=N_BPER)
    loss_bnd, bnd_stats = bw(model, Xb_full, Nb, uD=uD_full, gN=gN_full)

    loss_total = loss_data + loss_phys + loss_bnd
    loss_total.backward(); optimizer.step()

    if epoch % 1000 == 0 or epoch == 1:
        with torch.no_grad():
            sia_mse = (phys_stats["r_sia"]**2).mean().item()
            stk_mse = (phys_stats["r_stk"]**2).mean().item()
        print(f"[{epoch}/{EPOCHS}] tot={loss_total.item():.3e} data={loss_data.item():.3e} "
              f"phys={loss_phys.item():.3e} bnd={loss_bnd.item():.3e} "
              f"SIA_MSE={sia_mse:.3e} STK_MSE={stk_mse:.3e} lr={lr:.2e}")

# ============================================================
# Cell 6: Evaluation
# ============================================================
model.eval()

# Predictions (no grad)
with torch.no_grad():
    y_pred = model(X_test_tensor)
    y_pred_np = y_pred.detach().cpu().numpy()
    y_test_np = y_test_tensor.detach().cpu().numpy()

test_mse = mean_squared_error(y_test_np, y_pred_np)
r2 = r2_score(y_test_np, y_pred_np)
rmse = np.sqrt(test_mse)
mae = mean_absolute_error(y_test_np, y_pred_np)

print(f"MSE:  {test_mse:.6f}")
print(f"R2 Score: {r2:.6f}")
print(f"RMSE: {rmse:.6f}")
print(f"MAE: {mae:.6f}")

# Physics diagnostics
with torch.enable_grad():
    Xc_eval = sample_interior(4096)
    loss_phys_eval, _ = mf(model, Xc_eval, extra=None)
    r_sia_eval = mf.sia_residual(model, Xc_eval, extra=None)
    r_stk_eval = mf.stokes_reduced_residual(model, Xc_eval, body_force=None, nu=1.0)
    sia_mse_eval = (r_sia_eval**2).mean().item()
    stk_mse_eval = (r_stk_eval**2).mean().item()

print("\n=== Physics diagnostics (collocation) ===")
print(f"Weighted physics objective: {loss_phys_eval.item():.6f}")
print(f"SIA residual MSE:    {sia_mse_eval:.6e}")
print(f"Stokes residual MSE: {stk_mse_eval:.6e}")

# Plot: True vs Pred
plt.figure(figsize=(7,6))
plt.scatter(y_test_np, y_pred_np, s=5, alpha=0.5, label='Predictions')
mn = float(min(y_test_np.min(), y_pred_np.min()))
mx = float(max(y_test_np.max(), y_pred_np.max()))
plt.plot([mn, mx], [mn, mx], 'r--', label='Perfect fit')
plt.xlabel("True"); plt.ylabel("Predicted"); plt.title("True vs Predicted (Test)")
plt.legend(); plt.grid(True, linestyle='--', alpha=0.3); plt.tight_layout(); plt.show()

import numpy as np
y_pred_baseline_test = np.load("baseline_preds.npy")
print("✅ Loaded baseline predictions:", y_pred_baseline_test.shape)

# === 3-panel maps in METERS: GT vs Baseline vs Multi-Fidelity ===
import os, numpy as np, torch, matplotlib.pyplot as plt
from numpy import isfinite

def _to_np(a):
    return a.detach().cpu().numpy() if hasattr(a, "detach") else np.asarray(a)

# --- CONFIG: which columns in X_test are (x,y)? change if needed
IX, IY = 0, 1

# --- coords in PHYSICAL units (invert X scaler if present)
X_np = _to_np(X_test)
x_scaler = next((globals()[k] for k in ("scaler_X","x_scaler","X_scaler")
                 if k in globals() and globals()[k] is not None), None)
try:
    X_phys = x_scaler.inverse_transform(X_np.copy()) if x_scaler is not None else X_np
except Exception:
    X_phys = X_np
x, y = X_phys[:, IX].ravel(), X_phys[:, IY].ravel()

# --- ensure multi-fidelity predictions exist
model.eval()
with torch.no_grad():
    if 'y_pred_test' not in globals():
        Xt = torch.tensor(X_test, dtype=torch.float32).to(device)
        y_pred_test = model(Xt).cpu().numpy()

# --- baseline predictions: load if available, else placeholder so it still runs
if 'y_pred_baseline_test' not in globals():
    try:
        y_pred_baseline_test = np.load("baseline_preds.npy")
        print("Loaded baseline_preds.npy")
    except Exception:
        print("⚠️ Baseline preds not found. Using Multi-Fidelity as placeholder.")
        y_pred_baseline_test = _to_np(y_pred_test).copy()

# --- inverse-transform targets to METERS using your y scaler (auto-detect name)
y_true  = _to_np(y_test).reshape(-1,1)
y_predM = _to_np(y_pred_test).reshape(-1,1)
y_predB = _to_np(y_pred_baseline_test).reshape(-1,1)

y_scaler = next((globals()[k] for k in ("scaler_y","y_scaler","Y_scaler")
                 if k in globals() and globals()[k] is not None), None)

def to_meters(arr):
    if y_scaler is not None:
        try: return y_scaler.inverse_transform(arr).ravel()
        except Exception: pass
    return arr.ravel()  # already meters

z_true_m  = to_meters(y_true)
z_base_m  = to_meters(y_predB)
z_multi_m = to_meters(y_predM)

print("Meters ranges -> GT:", float(np.nanmin(z_true_m)), float(np.nanmax(z_true_m)),
      "| Baseline:", float(np.nanmin(z_base_m)), float(np.nanmax(z_base_m)),
      "| Multi-fid:", float(np.nanmin(z_multi_m)), float(np.nanmax(z_multi_m)))

# --- build a regular grid from scattered (x,y)
try:
    from scipy.interpolate import griddata
    use_scipy = True
except Exception:
    use_scipy = False

nx = ny = 600
xi = np.linspace(x.min(), x.max(), nx)
yi = np.linspace(y.min(), y.max(), ny)
Xg, Yg = np.meshgrid(xi, yi)

def interp_grid(z):
    if use_scipy:
        Z = griddata((x, y), z, (Xg, Yg), method="linear")
        m = np.isnan(Z)
        if m.any():
            Z[m] = griddata((x, y), z, (Xg[m], Yg[m]), method="nearest")
        return Z
    # nearest-only fallback
    idx = np.argmin((x[None,None,:]-Xg[:,:,None])**2 + (y[None,None,:]-Yg[:,:,None])**2, axis=2)
    return z[idx]

Zt, Zb, Zm = interp_grid(z_true_m), interp_grid(z_base_m), interp_grid(z_multi_m)

# --- shared color scale (robust)
vals = np.concatenate([Zt[isfinite(Zt)], Zb[isfinite(Zb)], Zm[isfinite(Zm)]])
vmin, vmax = np.percentile(vals, 2), np.percentile(vals, 98)

# --- 3 panels
fig, axes = plt.subplots(1, 3, figsize=(15, 5.2), constrained_layout=True)
for ax, (title, Z) in zip(axes, [
    ("(a) Ground Truth (meters)", Zt),
    ("(b) Baseline PINN (meters)", Zb),
    ("(c) Ours: Multi-Fidelity PINN (meters)", Zm),
]):
    im = ax.pcolormesh(Xg, Yg, Z, shading="auto", vmin=vmin, vmax=vmax)
    ax.set_title(title); ax.set_xlabel("X"); ax.set_ylabel("Y"); ax.set_aspect('equal', adjustable='box')
    cb = fig.colorbar(im, ax=ax, shrink=0.85); cb.set_label("Bed Elevation (m)")

# --- error map (true − multi-fid) in meters
err = Zt - Zm
ev = np.max(np.abs(np.percentile(err[isfinite(err)], [2, 98])))
fig_e, ax = plt.subplots(figsize=(5.8, 5.2))
ime = ax.pcolormesh(Xg, Yg, err, shading="auto", vmin=-ev, vmax=+ev)
ax.set_title("Error Map (true − multi-fid)"); ax.set_xlabel("X"); ax.set_ylabel("Y"); ax.set_aspect('equal', adjustable='box')
cbe = fig_e.colorbar(ime, ax=ax, shrink=0.9); cbe.set_label("Meters")

ax.ticklabel_format(style='plain', axis='x')  # keep plain numbers
plt.xticks(rotation=45)                       # rotate labels so they don’t overlap


# --- save
os.makedirs("figs", exist_ok=True)
fig.savefig("figs/spatial_3panel_meters.png", dpi=300, bbox_inches="tight")
fig_e.savefig("figs/spatial_error_meters.png", dpi=300, bbox_inches="tight")
print("Saved:", os.path.abspath("figs/spatial_3panel_meters.png"),
      "and", os.path.abspath("figs/spatial_error_meters.png"))